name: Fetch BigBasket Product Data

on:
  workflow_dispatch:

jobs:
  fetch-products:
    runs-on: ubuntu-latest

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_TARGET_USER_ID: ${{ secrets.USER_ID }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      # ✅ Cache pip dependencies
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      # ✅ Cache Playwright browsers
      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}

      # ✅ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      # ✅ Install Python dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # ✅ Install Playwright browsers (will reuse cache if available)
      - name: Install Playwright Browsers
        run:  python -m playwright install chromium

      - name: Debug env vars
        run: |
          echo "✅ Supabase URL: $SUPABASE_URL"
          echo "✅ Supabase Key Length: ${#SUPABASE_SERVICE_ROLE_KEY}"
          echo "✅ Supabase User ID: $SUPABASE_TARGET_USER_ID"

      - name: Fetch session data from Supabase
        run: python fetch_from_supabase.py

      - name: Convert bb.json for Playwright
        run: python convert_bbjson.py

      - name: Run BigBasket scraper
        run: python bb_scraper.py

      - name: Upload product_data.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: bigbasket-product-data
          path: product_data.json
