name: Fetch BigBasket Product Data

on:
  workflow_dispatch:

jobs:
  fetch-products:
    runs-on: ubuntu-latest

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_TARGET_USER_ID: ${{ secrets.USER_ID }}
      PLAYWRIGHT_BROWSERS_PATH: ~/.cache/ms-playwright

    steps:
      # ✅ Checkout code
      - name: Checkout repo
        uses: actions/checkout@v3

      # ✅ Cache pip and Playwright Chromium
      - name: Cache Python & Playwright
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/ms-playwright
          key: deps-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            deps-${{ runner.os }}-

      # ✅ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      # ✅ Install Python dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # ✅ Install Chromium browser in cache path
      - name: Install Playwright Chromium
        run: |
          PLAYWRIGHT_BROWSERS_PATH=$PLAYWRIGHT_BROWSERS_PATH \
          python -m playwright install chromium

      # ✅ Fetch session data from Supabase
      - name: Fetch session data
        run: python fetch_from_supabase.py

      # ✅ Convert to Playwright storage_state
      - name: Convert session to storage_state
        run: python convert_bbjson.py

      # ✅ Scrape BigBasket data
      - name: Run BigBasket scraper
        run: |
          PLAYWRIGHT_BROWSERS_PATH=$PLAYWRIGHT_BROWSERS_PATH \
          python bb_scraper.py

      # ✅ Upload results
      - name: Upload product_data.json
        uses: actions/upload-artifact@v4
        with:
          name: bigbasket-product-data
          path: product_data.json
