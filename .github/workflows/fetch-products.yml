name: Fetch BigBasket Product Data

on:
  workflow_dispatch:

jobs:
  fetch-products:
    runs-on: ubuntu-latest

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_TARGET_USER_ID: ${{ secrets.USER_ID }}

    steps:
      # ✅ Checkout code
      - name: Checkout repo
        uses: actions/checkout@v3

      # ✅ Cache Python + Playwright dependencies
      - name: Cache pip & Playwright
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/ms-playwright
          key: deps-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            deps-${{ runner.os }}-

      # ✅ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      # ✅ Install Python packages
      - name: Install dependencies
        run: pip install  --no-deps --no-cache-dir --disable-pip-version-check -r requirements.txt

      # ✅ Install Playwright browsers (chromium only)
      - name: Install Chromium via Playwright (cached)
        run: |
          PLAYWRIGHT_BROWSERS_PATH=$HOME/.cache/ms-playwright \
          python -m playwright install chromium

     

      # ✅ Fetch session cookie from Supabase
      - name: Fetch session data from Supabase
        run: python fetch_from_supabase.py

      # ✅ Convert session to Playwright format
      - name: Convert bb.json for Playwright
        run: python convert_bbjson.py

      # ✅ Run the BigBasket scraper
      - name: Run BigBasket scraper
        run: |
          PLAYWRIGHT_BROWSERS_PATH=$HOME/.cache/ms-playwright \
          python bb_scraper.py

      # ✅ Upload output
      - name: Upload product_data.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: bigbasket-product-data
          path: product_data.json
